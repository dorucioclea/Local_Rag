from pydantic import BaseModel
import yaml
from angle_emb import AnglE, Prompts
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import numpy as np
import requests
import torch
import json


# Data model for a single string to make an embedding result
class EmbDataModel(BaseModel):
    prompt: str


# Data model for a batch of strings to make an embedding batch
class BatchEmbModel(BaseModel):
    prompt_batch: list


# Prompt data model for a prompt into the LLM
class PromptData(BaseModel):
    model: str
    stream: bool
    system: str
    temperature: int
    prompt: str


class OllamaLLM:
    """
    The OllamaLLM class is used for making chat requests to a language model API. It takes in a model, URL, temperature, and a stream flag as parameters.

    Attributes:
        model (str): The name of the language model to use.
        url (str): The URL of the language model API endpoint. Default is "http://localhost:11434/api/generate".
        temperature (int): The temperature value to be used in the generation process. Default is 0.
        stream (bool): A flag indicating if the response should be streamed or not. Default is False.

    Methods:
        chat_request(prompt_in: str, context: str, content_gen: bool) -> str:
            Makes a chat request to the language model API with the given prompt and context.

            Parameters:
                prompt_in (str): The prompt or question to be answered by the language model.
                context (str): The context or information to be used by the language model to answer the prompt.
                content_gen (bool): A flag indicating if the request is for content generation or question answering.

            Returns:
                str: The response generated by the language model API.
    """

    def __init__(self, config_file):
        with open(config_file, 'r') as stream:
            data_loaded = yaml.safe_load(stream)
        self.model = data_loaded["model_name"]
        self.url = data_loaded["ollama_api_url"]
        self.stream = data_loaded["stream"]
        self.temperature = data_loaded["temperature"]
        self.system = "You use primarily the supplied information to answer question or query. If there isn't enough information supplied to properly answer the question, say you don't know the answer."

    def chat_request(self, prompt_in: str, context: str, content_gen: bool) -> str:
        # Select a prompt based on question answering or content generation
        if content_gen:
            prompt = f"You are an assistant for generating short form content addressing the problem. You use the retrieved context to generate short form content addressing the problem.\nGenerate content that addresses the problem: {prompt_in}\nUsing the following retrieved context: {context}.\nAnswer:"
        else:
            prompt = f"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {prompt_in}\nContext: {context}.\nAnswer:"

        # Data model for entry into LLM
        data_in = PromptData(
            model=self.model,
            stream=self.stream,
            prompt=prompt,
            system=self.system,
            temperature=self.temperature
        )

        # LLM post request
        response = requests.post(self.url, json=data_in.model_dump())

        return json.loads(response.text)["response"]


class EmbeddingClass:
    """
    EmbeddingClass

    This class provides methods for embedding text data using a pre-trained model.

    Methods:
        - __init__(): Initialize the EmbeddingClass object and load the pre-trained model.
        - batch_embedding(prompt_batch: list) -> list: Embeds a batch of prompts using the pre-trained model.
        - return_embedding(prompt: str) -> list: Embeds a single prompt using the pre-trained model.

    Attributes:
        - _model: The pre-trained model used for embedding.

    """

    def __init__(self, config_file):
        with open(config_file, 'r') as stream:
            data_loaded = yaml.safe_load(stream)
        self._batch_size = data_loaded["embedding_batches"]
        self._model = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()
        self._model.set_prompt(prompt=Prompts.C)

    def batch_embedding(self, prompt_batch: list) -> list:
        # Split the prompt_batch into batches of 64 entries into embedding
        prompt_chunks = [prompt_batch[i:i + self._batch_size] for i in range(0, len(prompt_batch), self._batch_size)]

        # Process each chunk and accumulate the results
        batch_encodings = []
        for chunk in prompt_chunks:
            data_in = BatchEmbModel(prompt_batch=chunk)
            chunk_encoding = self._model.encode(data_in.prompt_batch, to_numpy=True)
            chunk_encoding = chunk_encoding.tolist()
            batch_encodings.extend(chunk_encoding)

        return batch_encodings

    def return_embedding(self, prompt: str) -> list:
        data_in = EmbDataModel(prompt=prompt)
        embedding = self._model.encode({"text": data_in.prompt}, to_numpy=True)[0]

        return embedding


class EmbeddingReranker:
    """
    This class represents a re-ranker for embedding-based retrieval models.

    Attributes:
        _tokenizer (AutoTokenizer): The tokenizer used for encoding text inputs.
        _model (AutoModelForSequenceClassification): The model used for reranking.
        _device (torch.device): The device used for running the model (GPU or CPU).

    Methods:
        rerank_data(data_in: RerankDataModel) -> list:
            Re-ranks a list of sources based on a given query.

    """

    def __init__(self):
        self._tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')
        self._model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')
        self._device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self._model.to(self._device)

    def rerank_data(self, query: str, sources: list) -> list:
        # Create pairs for the query
        pairs = [[query, source] for source in sources]

        # Run the model
        self._model.eval()
        with torch.no_grad():
            inputs = self._tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)
            inputs = inputs.to(self._device)

            scores = self._model(**inputs, return_dict=True).logits.view(-1, ).float()
            scores = scores.cpu().numpy()

        # Sort the sources
        sorted_index_sources = np.argsort(scores).tolist()
        sorted_index_sources = sorted_index_sources[::-1]
        sorted_sources = [sources[i] for i in sorted_index_sources]

        return sorted_sources
